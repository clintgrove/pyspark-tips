{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "errorTable = \"dbo.Error_Table\"\n",
        "auditTable = \"dbo.Audit_Log_tbl\"\n",
        "controlTable = \"dbo.STG_Control_Table\"\n",
        "audit_data = []\n",
        "encrypt = \"true\"\n",
        "host_name_in_certificate = \"*.database.windows.net\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clintDevpool",
              "session_id": "9",
              "statement_id": 255,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-06-02T15:54:21.3279019Z",
              "session_start_time": null,
              "execution_start_time": "2023-06-02T15:54:21.4958309Z",
              "execution_finish_time": "2023-06-02T15:54:21.6590365Z",
              "spark_jobs": null,
              "parent_msg_id": "ee207eab-6205-472d-982d-70dda77c65b0"
            },
            "text/plain": "StatementMeta(clintDevpool, 9, 255, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 254,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Initialize variables",
          "showTitle": true,
          "nuid": "15b0b16f-93f0-47ee-aac7-fd29105b4d24"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import msal\r\n",
        "from notebookutils import mssparkutils\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql import functions as F\r\n",
        "import datetime as dt"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clintDevpool",
              "session_id": "9",
              "statement_id": 256,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-06-02T15:54:21.4001442Z",
              "session_start_time": null,
              "execution_start_time": "2023-06-02T15:54:21.8321081Z",
              "execution_finish_time": "2023-06-02T15:54:22.007517Z",
              "spark_jobs": null,
              "parent_msg_id": "a11f7bb3-a51e-410a-b58d-cf17ff75de5d"
            },
            "text/plain": "StatementMeta(clintDevpool, 9, 256, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 255,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "nuid": "43452c9b-510c-48d1-8190-5b8a39c9869e"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_principal_id = \"spn-ttia-01-appid\"\r\n",
        "service_principal_secret = \"spn-ttia-01-ServicePrincipalSecret\"\r\n",
        "tenant_id = \"bhf-ttia-tenant-id\"\r\n",
        "azure_sql_url = \"sql-ttia-01-jdbcsqlserver\"\r\n",
        "database_name = \"sql-ttia-01-database\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clintDevpool",
              "session_id": "9",
              "statement_id": 257,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-06-02T15:54:21.4762213Z",
              "session_start_time": null,
              "execution_start_time": "2023-06-02T15:54:22.1596095Z",
              "execution_finish_time": "2023-06-02T15:54:22.3728683Z",
              "spark_jobs": null,
              "parent_msg_id": "a36dbaa4-b98c-4727-9158-d651aac3ac6f"
            },
            "text/plain": "StatementMeta(clintDevpool, 9, 257, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 256,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getToken():\r\n",
        "    from notebookutils import mssparkutils\r\n",
        "    mssparkutils.credentials.getSecret\r\n",
        "    \r\n",
        "    resource_app_id_url = \"https://database.windows.net/.default\"\r\n",
        "    service_principal_id_DB = mssparkutils.credentials.getSecretWithLS(\"LS_AzureKeyVault\" , service_principal_id)\r\n",
        "    service_principal_secret_DB = mssparkutils.credentials.getSecretWithLS(\"LS_AzureKeyVault\" , service_principal_secret)\r\n",
        "    tenant_id_DB = mssparkutils.credentials.getSecretWithLS(\"LS_AzureKeyVault\" , tenant_id)\r\n",
        "    azure_sql_url_DB = mssparkutils.credentials.getSecretWithLS(\"LS_AzureKeyVault\" , azure_sql_url)\r\n",
        "    database_name_DB = mssparkutils.credentials.getSecretWithLS(\"LS_AzureKeyVault\" , database_name)\r\n",
        "    authority = \"https://login.microsoftonline.com/\" + tenant_id_DB\r\n",
        "\r\n",
        "    # Generating access token using the service principal\r\n",
        "    context = msal.ConfidentialClientApplication(service_principal_id_DB, service_principal_secret_DB, authority)\r\n",
        "    accounts = context.get_accounts(service_principal_id_DB)\r\n",
        "    token = context.acquire_token_silent_with_error([resource_app_id_url], account=None,force_refresh=True)\r\n",
        "    if token:\r\n",
        "        print(\"Result Found!\")\r\n",
        "    if not token:\r\n",
        "        print(\"No suitable token exists in cache. Let's get a new one from AAD.\")\r\n",
        "        token = context.acquire_token_for_client(resource_app_id_url)\r\n",
        "\r\n",
        "    access_token9 = token[\"access_token\"]\r\n",
        "\r\n",
        "    return access_token9, azure_sql_url_DB, database_name_DB\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clintDevpool",
              "session_id": "9",
              "statement_id": 258,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-06-02T15:54:21.8255011Z",
              "session_start_time": null,
              "execution_start_time": "2023-06-02T15:54:22.5409702Z",
              "execution_finish_time": "2023-06-02T15:54:22.7052365Z",
              "spark_jobs": null,
              "parent_msg_id": "61043b88-f6f6-4b5c-825f-01646f1a96c1"
            },
            "text/plain": "StatementMeta(clintDevpool, 9, 258, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 257,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_table(query):\n",
        "    tokenvar, azure_sql_url_DB, database_name_DB = getToken()\n",
        "    df = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", azure_sql_url_DB) \\\n",
        "    .option(\"databaseName\", database_name_DB) \\\n",
        "    .option(\"accessToken\", tokenvar) \\\n",
        "    .option(\"query\",query) \\\n",
        "    .option(\"encrypt\", encrypt) \\\n",
        "    .option(\"hostNameInCertificate\", host_name_in_certificate) \\\n",
        "    .option(\"msiTokenCacheTtl\", 0) \\\n",
        "    .load()\n",
        "    return df"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clintDevpool",
              "session_id": "9",
              "statement_id": 259,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-06-02T15:54:22.056882Z",
              "session_start_time": null,
              "execution_start_time": "2023-06-02T15:54:22.8503354Z",
              "execution_finish_time": "2023-06-02T15:54:23.0294769Z",
              "spark_jobs": null,
              "parent_msg_id": "3e079c2c-8e59-42db-83a3-f2d1b90392f4"
            },
            "text/plain": "StatementMeta(clintDevpool, 9, 259, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 258,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Query data from table",
          "showTitle": true,
          "nuid": "0f5c1ca0-ee5e-40c5-85a1-7bd2cc974363"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_data(df,table):\n",
        "    tokenvar, azure_sql_url_DB, database_name_DB = getToken()\n",
        "    df.write.mode(\"append\") \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", azure_sql_url_DB) \\\n",
        "    .option(\"dbtable\", table) \\\n",
        "    .option(\"databaseName\", database_name_DB) \\\n",
        "    .option(\"accessToken\", tokenvar) \\\n",
        "    .option(\"encrypt\", encrypt) \\\n",
        "    .option(\"hostNameInCertificate\", host_name_in_certificate) \\\n",
        "    .option(\"msiTokenCacheTtl\", 0) \\\n",
        "    .save()\n",
        "    display(df)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clintDevpool",
              "session_id": "9",
              "statement_id": 260,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-06-02T15:54:22.1817918Z",
              "session_start_time": null,
              "execution_start_time": "2023-06-02T15:54:23.2159736Z",
              "execution_finish_time": "2023-06-02T15:54:23.4053751Z",
              "spark_jobs": null,
              "parent_msg_id": "c3896861-62d3-40bd-bb71-078d2fdccc41"
            },
            "text/plain": "StatementMeta(clintDevpool, 9, 260, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 259,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Insert data into table",
          "showTitle": true,
          "nuid": "f02aa52b-ae8a-404f-bcdd-5ce6e7bce15e"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createAuditDataframe(audit_data):\n",
        "    schema = StructType([ \\\n",
        "            StructField(\"PipelineName\",StringType(),True), \\\n",
        "            StructField(\"RunId\",StringType(),True), \\\n",
        "            StructField(\"CreatedOn\",TimestampType(),True), \\\n",
        "            StructField(\"TableName\", StringType(), True), \\\n",
        "            StructField(\"Filepath\",StringType(),True), \\\n",
        "            StructField(\"Status\",StringType(),True), \\\n",
        "            StructField(\"Errormessage\",StringType(),True), \\\n",
        "            StructField(\"BadRecordsPath\", StringType(), True), \\\n",
        "            StructField(\"StartTime\",TimestampType(),True), \\\n",
        "            StructField(\"EndTime\",TimestampType(),True), \\\n",
        "            StructField(\"CopyDuration\",StringType(),True), \\\n",
        "            StructField(\"DataRead\", IntegerType(), True), \\\n",
        "            StructField(\"DataWritten\",IntegerType(),True), \\\n",
        "            StructField(\"Throughput\",StringType(),True), \\\n",
        "            StructField(\"Fileprocessed\",StringType(),True), \\\n",
        "            StructField(\"ProcessType\",StringType(),True), \\\n",
        "          ])    \n",
        "    \n",
        "    audit_df = spark.createDataFrame(data=audit_data,schema=schema)\n",
        "    audit_df.printSchema()\n",
        "    return audit_df"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clintDevpool",
              "session_id": "9",
              "statement_id": 261,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-06-02T15:54:22.3852217Z",
              "session_start_time": null,
              "execution_start_time": "2023-06-02T15:54:23.5513911Z",
              "execution_finish_time": "2023-06-02T15:54:23.7142368Z",
              "spark_jobs": null,
              "parent_msg_id": "99b5980e-85e8-4d92-b213-d35af0cc8c07"
            },
            "text/plain": "StatementMeta(clintDevpool, 9, 261, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 260,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Create audit dataframe",
          "showTitle": true,
          "nuid": "26f717f5-c210-4642-ada7-068db2e2fe53"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createErrorDataframe(error_data):\n",
        "    schema = StructType([ \\\n",
        "            StructField(\"EntityName\",StringType(),True), \\\n",
        "            StructField(\"FileName\",StringType(),True), \\\n",
        "            StructField(\"Status\",StringType(),True), \\\n",
        "            StructField(\"RunId\", StringType(), True), \\\n",
        "            StructField(\"ErrorDescription\", StringType(), True), \\\n",
        "            StructField(\"CreatedOn\", TimestampType(), True), \\\n",
        "            StructField(\"ErrorType\", StringType(), True), \\\n",
        "            StructField(\"BadRecordsPath\", StringType(), True), \\\n",
        "          ])    \n",
        "\n",
        "    error_df = spark.createDataFrame(data=error_data,schema=schema)\n",
        "    error_df.printSchema()\n",
        "    return error_df"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clintDevpool",
              "session_id": "9",
              "statement_id": 262,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-06-02T15:54:22.5433638Z",
              "session_start_time": null,
              "execution_start_time": "2023-06-02T15:54:23.8723259Z",
              "execution_finish_time": "2023-06-02T15:54:24.0350908Z",
              "spark_jobs": null,
              "parent_msg_id": "bb46e92a-524d-40c2-9226-91c5ab5de2f0"
            },
            "text/plain": "StatementMeta(clintDevpool, 9, 262, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 261,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Create error dataframe",
          "showTitle": true,
          "nuid": "a4c61e63-4cd7-4898-8731-18b2ae76d26f"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resource_app_id_url = \"https://database.windows.net/.default\"\r\n",
        "# service_principal_id_DB = mssparkutils.credentials.getSecretWithLS(\"LS_AzureKeyVault\" , service_principal_id)\r\n",
        "# service_principal_secret_DB = mssparkutils.credentials.getSecretWithLS(\"LS_AzureKeyVault\" , service_principal_secret)\r\n",
        "# tenant_id_DB = mssparkutils.credentials.getSecretWithLS(\"LS_AzureKeyVault\" , tenant_id)\r\n",
        "# azure_sql_url_DB = mssparkutils.credentials.getSecretWithLS(\"LS_AzureKeyVault\" , azure_sql_url)\r\n",
        "# database_name_DB = mssparkutils.credentials.getSecretWithLS(\"LS_AzureKeyVault\" , database_name)\r\n",
        "# authority = \"https://login.microsoftonline.com/\" + tenant_id_DB\r\n",
        "# context = msal.ConfidentialClientApplication(service_principal_id_DB, service_principal_secret_DB, authority)\r\n",
        "# account2 = context.get_accounts(username=None)\r\n",
        "# print(account2)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "clintDevpool",
              "session_id": "9",
              "statement_id": 263,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-06-02T15:54:22.709309Z",
              "session_start_time": null,
              "execution_start_time": "2023-06-02T15:54:24.2104105Z",
              "execution_finish_time": "2023-06-02T15:54:24.3756376Z",
              "spark_jobs": null,
              "parent_msg_id": "1cd6149c-49d8-4a7d-84ae-4e47ee9d925d"
            },
            "text/plain": "StatementMeta(clintDevpool, 9, 263, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "execution_count": 262,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import datetime as dt\r\n",
        "# error_data=[]\r\n",
        "# createdOn = dt.datetime.now()\r\n",
        "# error_data.append((\"testentity\",\"\",\"Success\",\"-1191\",\"Staging to Processed Data Load\",createdOn,\"Staging to Processed Data Load\",\"blah\"))\r\n",
        "# error_df = createErrorDataframe(error_data)\r\n",
        "# insert_data(error_df,errorTable)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}